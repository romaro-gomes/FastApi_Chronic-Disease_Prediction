{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import(\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (classification_report,\n",
    "                              confusion_matrix,\n",
    "                              accuracy_score,\n",
    "                              ConfusionMatrixDisplay,\n",
    "                              roc_auc_score,\n",
    "                              RocCurveDisplay,\n",
    "                              precision_score,\n",
    "                              recall_score)\n",
    "from sklearn.model_selection import KFold,train_test_split,ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../data/stroke_impute_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop('stroke',axis=1)\n",
    "y=data['stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgk = KFold(n_splits=5,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "                \"Random_Forest\": RandomForestClassifier(random_state=42),\n",
    "                \"Decision_Tree\": DecisionTreeClassifier(random_state=42),\n",
    "                \"Gradient_Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "                \"Logistic_Regression\": LogisticRegression(random_state=42),\n",
    "                \"XGBClassifier\": XGBClassifier(),\n",
    "                \"AdaBoost_Classifier\": AdaBoostClassifier(random_state=42,),\n",
    "                \"SVM_Classifier\":svm.SVC(random_state=42,)\n",
    "            }\n",
    "params={\n",
    "                \"Decision_Tree\": {\n",
    "                    'criterion':['gini', 'log_loss', 'entropy'],\n",
    "                    'max_features':['auto', 'sqrt', 'log2'],\n",
    "                    'max_depth' : [3,5,10]\n",
    "\n",
    "                },\n",
    "                \"Random_Forest\":{                    \n",
    "                    'n_estimators': [8,16,32],\n",
    "                    'criterion':['gini','entropy','log_loss'],\n",
    "                    'max_depth' : [3,5,10],\n",
    "\n",
    "                },\n",
    "                \"Gradient_Boosting\":{\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'n_estimators': [8,16,32],\n",
    "                    'loss' : ['log_loss'],\n",
    "                    'max_depth' : [3,5,10],\n",
    "                },\n",
    "\n",
    "               \"Logistic_Regression\":{\n",
    "                   'penalty' : ['l1', 'l2', 'elasticnet', None],\n",
    "                    'C':[1,2,5],\n",
    "\n",
    "               },\n",
    "                \"XGBClassifier\":{\n",
    "                    'learning_rate':[.1,.01,.05,.001],\n",
    "                    'n_estimators': [8,16,32]\n",
    "                },\n",
    "                \"AdaBoost_Classifier\":{\n",
    "                    'learning_rate':[.1,.01,0.5,.001],\n",
    "                    'n_estimators': [8,16,32]\n",
    "                },\n",
    "                \"SVM_Classifier\":{\n",
    "                     'C':[1,2,5],\n",
    "                     'kernel' : ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "                }\n",
    "                \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_pasta=f'Kfold_experiments_stroke_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "os.makedirs(caminho_pasta,exist_ok=True)\n",
    "os.environ['MLFLOW_ARTIFACT_ROOT']=caminho_pasta\n",
    "mlflow.set_experiment('K_Fold_Experimental_stroke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list(models))):\n",
    "    model=list(models.values())[i]\n",
    "    model_name=list(models.keys())[i]\n",
    "\n",
    "    print(f'------------------Trainning {model_name} Model--------------------')\n",
    "    model_params=params.get(model_name,{})\n",
    "    param_grid=list(ParameterGrid(model_params))\n",
    "    loop=0\n",
    "    for params_combination in param_grid:\n",
    "        loop +=1\n",
    "        try:\n",
    "            with mlflow.start_run():\n",
    "                j=1\n",
    "\n",
    "                metrics_acc=[]\n",
    "                metrics_auc=[]\n",
    "                metrics_pred_0=[]\n",
    "                metrics_pred_1=[]\n",
    "                metrics_recall_0=[]\n",
    "                metrics_recall_1=[]\n",
    "\n",
    "                for train_id,test_id in sgk.split(X):\n",
    "                    mlflow.log_param('model',model_name)\n",
    "                    mlflow.log_params(params_combination)\n",
    "                    print(f'{model_name} Fold {j}:')\n",
    "\n",
    "                    X_val=X.iloc[test_id]\n",
    "                    y_val=y.iloc[test_id]\n",
    "\n",
    "                    X_train=X.iloc[train_id]\n",
    "                    y_train=y.iloc[train_id]\n",
    "\n",
    "                    model.set_params(**params_combination) \n",
    "                    model.fit(X_train,y_train)\n",
    "\n",
    "                    pred=model.predict(X_val)\n",
    "\n",
    "                    acc_score=accuracy_score(y_val,pred)\n",
    "                    metrics_acc.append(acc_score)\n",
    "\n",
    "                    auc_score= roc_auc_score(y_val,pred)\n",
    "                    metrics_auc.append(auc_score)\n",
    "\n",
    "                    precision0=precision_score(y_val, pred, average='binary',pos_label=0)\n",
    "                    metrics_pred_0.append(precision0)\n",
    "\n",
    "                    precision1=precision_score(y_val, pred, average='binary',pos_label=1)\n",
    "                    metrics_pred_1.append(precision1)\n",
    "\n",
    "\n",
    "                    recall0=recall_score(y_val,pred,average='binary',pos_label=0)\n",
    "                    metrics_recall_0.append(recall0)\n",
    "\n",
    "                    recall1=recall_score(y_val,pred,average='binary',pos_label=1)\n",
    "                    metrics_recall_1.append(recall1)                    \n",
    "                    j+=1\n",
    "                \n",
    "                mlflow.log_metric(\"acc_score\",np.mean(metrics_acc))\n",
    "                mlflow.log_metric(\"AUC Score\",np.mean(metrics_auc))\n",
    "                mlflow.log_metric(f\"Precision Score 0\",np.mean(metrics_pred_0))\n",
    "                mlflow.log_metric(f\"Precision Score 1\",np.mean(metrics_pred_1))\n",
    "                mlflow.log_metric(f\"Recall Score 0\",np.mean(metrics_recall_0))\n",
    "                mlflow.log_metric(f\"Recall Score 1\",np.mean(metrics_recall_1))          \n",
    "\n",
    "                mlflow.end_run()\n",
    "                            \n",
    "        except Exception as e:\n",
    "            log_print=f'Error in{model_name} with prameter: {params_combination}: {str(e)}'\n",
    "            print(log_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingClassifier(learning_rate=0.1,loss='log_loss',max_depth=10,n_estimators=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_pasta='Stroke_Model'\n",
    "os.makedirs(caminho_pasta,exist_ok=True)\n",
    "os.environ['MLFLOW_ARTIFACT_ROOT']=caminho_pasta\n",
    "mlflow.set_experiment(caminho_pasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "            with mlflow.start_run():\n",
    "                    model.fit(X,y)\n",
    "                    mlflow.sklearn.log_model(model,caminho_pasta)\n",
    "\n",
    "                    pred=model.predict(X)\n",
    "\n",
    "                    acc_score=accuracy_score(y,pred)\n",
    "                    mlflow.log_metric(\"acc_score\",acc_score)\n",
    "\n",
    "                    auc_score= roc_auc_score(y,pred)\n",
    "                    mlflow.log_metric(\"AUC Score\",auc_score)\n",
    "\n",
    "                    precision0=precision_score(y, pred, average='binary',pos_label=0)                    \n",
    "                    precision1=precision_score(y, pred, average='binary',pos_label=1)\n",
    "                    \n",
    "\n",
    "                    recall0=recall_score(y,pred,average='binary',pos_label=0)                  \n",
    "                    recall1=recall_score(y,pred,average='binary',pos_label=1)               \n",
    "                \n",
    "                    mlflow.log_metric(f\"Precision Score 0\",precision0)\n",
    "                    mlflow.log_metric(f\"Precision Score 1\",precision1)\n",
    "                    mlflow.log_metric(f\"Recall Score 0\",recall0)\n",
    "                    mlflow.log_metric(f\"Recall Score 1\",recall1)   \n",
    "\n",
    "                    report_testing = classification_report(y, pred, output_dict=True)\n",
    "                    df_report_testing = pd.DataFrame(report_testing).transpose()\n",
    "\n",
    "                    artifact_path=os.path.join(caminho_pasta,f'report_testing.csv')        \n",
    "                    df_report_testing.to_csv(artifact_path)\n",
    "                    mlflow.log_artifact(artifact_path)\n",
    "\n",
    "                    cm=confusion_matrix(y,pred,labels=model.classes_)\n",
    "                    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
    "                    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                    disp.plot(ax=ax, values_format='d', cmap='BuPu')\n",
    "                    ax.set_title(f'Stroke Model')\n",
    "\n",
    "                    artifact_path=os.path.join(caminho_pasta,\"_conf_matrix.png\")\n",
    "                    plt.savefig(artifact_path)\n",
    "                    mlflow.log_artifact(artifact_path)\n",
    "\n",
    "                    \n",
    "                    display=RocCurveDisplay.from_predictions(y,pred)\n",
    "                    plt.title(\"RocCurve GB\")\n",
    "                    display.plot()\n",
    "                    artifact_path=os.path.join(caminho_pasta,\"roc_curve.png\")\n",
    "                    plt.savefig(artifact_path)\n",
    "                    mlflow.log_artifact(artifact_path)\n",
    "                          \n",
    "\n",
    "                    mlflow.end_run()\n",
    "                            \n",
    "except Exception as e:\n",
    "            log_print=f'Error in training GB'\n",
    "            print(log_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
